{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#help taken from anirudh topiwala git-repo\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    return np.reshape(state, [1, 4])\n",
    "\n",
    "def replay(model,gamma,memory, batch_size):\n",
    "    \n",
    "    x_batch, y_batch = [], []\n",
    "    #sample a random mini batch out of the memory stored in previous episode\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target = model.predict(state)\n",
    "        y_target[0][action] = reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "\n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 14.0 streaks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 10.86 streaks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 19.47 streaks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 22.07 streaks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 37.33 streaks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 57.92 streaks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 69.04 streaks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 89.77 streaks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 88.21 streaks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 138.46 streaks.\n",
      "[Episode 1000] - Mean survival time over last 100 episodes was 139.54 streaks.\n",
      "[Episode 1100] - Mean survival time over last 100 episodes was 126.37 streaks.\n",
      "[Episode 1200] - Mean survival time over last 100 episodes was 159.32 streaks.\n",
      "Ran 1297 episodes. Solved after 1197 trials ✔\n"
     ]
    }
   ],
   "source": [
    "#  all the parameters needed\n",
    "n_episodes=7000\n",
    "n_win_ticks=200\n",
    "# can have any number of steps per episode thus none\n",
    "max_env_steps=None\n",
    "gamma=0.99\n",
    "epsilon=1.0 \n",
    "epsilon_min=0.01\n",
    "epsilon_decay=0.995\n",
    "alpha=0.01\n",
    "alpha_decay=0.01 \n",
    "batch_size=64 \n",
    "monitor=False \n",
    "quiet=False\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "model.add(Dense(36, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "\n",
    "\n",
    "# environment setting \n",
    "memory = deque(maxlen=100000)\n",
    "env = gym.make('CartPole-v1')\n",
    "if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "\n",
    "scores = deque(maxlen=100)\n",
    "solved = 0 \n",
    "for e in range(n_episodes):\n",
    "    state = preprocess_state(env.reset())\n",
    "    done = False\n",
    "    i = 0\n",
    "    #run completely for one complete game for instance \n",
    "    #assuming some model for q given state and action initially the the model updates each time it runs\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        #for exploration\n",
    "        eps = max(epsilon_min, min(epsilon, 1.0 - math.log10((e + 1) * epsilon_decay)))\n",
    "        action = env.action_space.sample() if (np.random.random() <= eps) else np.argmax(model.predict(state))\n",
    "        \n",
    "        #Next state prediction\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        memory.append((state, action, reward, next_state, done)) \n",
    "        state = next_state\n",
    "        i += 1\n",
    "\n",
    "    # score basically it the number of steps per episode it has survived thus has been assigned to i which is \n",
    "    # incremented every time step it is not done\n",
    "\n",
    "    scores.append(i)\n",
    "    mean_score = np.mean(scores)\n",
    "    if mean_score >= n_win_ticks and e >= 100:\n",
    "        if not quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
    "        solved = 1\n",
    "        break\n",
    "            \n",
    "    if e % 100 == 0 and not quiet:\n",
    "        print('[Episode {}] - Mean survival time over last 100 episodes was {} streaks.'.format(e, mean_score))\n",
    "\n",
    "    replay(model,gamma,memory,batch_size)\n",
    "    \n",
    "    #reduce the epsilon over episode so that we reduce the exploration and increase exploitation over time \n",
    "    if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "if not quiet and solved == 0: print('Did not solve after {} episodes'.format(e))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 316 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 90 timesteps\n",
      "Episode finished after 232 timesteps\n",
      "Episode finished after 405 timesteps\n",
      "Episode finished after 82 timesteps\n",
      "Episode finished after 269 timesteps\n",
      "Episode finished after 256 timesteps\n",
      "Episode finished after 186 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 92 timesteps\n",
      "Episode finished after 84 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 213 timesteps\n",
      "Episode finished after 114 timesteps\n",
      "Episode finished after 248 timesteps\n",
      "Episode finished after 218 timesteps\n",
      "Episode finished after 78 timesteps\n",
      "Episode finished after 294 timesteps\n",
      "Episode finished after 96 timesteps\n",
      "Episode finished after 105 timesteps\n",
      "Episode finished after 301 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 245 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 128 timesteps\n",
      "Episode finished after 191 timesteps\n",
      "Episode finished after 109 timesteps\n",
      "Episode finished after 192 timesteps\n",
      "Episode finished after 181 timesteps\n",
      "Episode finished after 218 timesteps\n",
      "Episode finished after 96 timesteps\n",
      "Episode finished after 162 timesteps\n",
      "Episode finished after 89 timesteps\n",
      "Episode finished after 241 timesteps\n",
      "Episode finished after 480 timesteps\n",
      "Episode finished after 267 timesteps\n",
      "Episode finished after 134 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 91 timesteps\n",
      "Episode finished after 404 timesteps\n",
      "Episode finished after 103 timesteps\n",
      "Episode finished after 313 timesteps\n",
      "Episode finished after 104 timesteps\n",
      "Episode finished after 161 timesteps\n",
      "Episode finished after 169 timesteps\n",
      "Episode finished after 236 timesteps\n",
      "Episode finished after 88 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 120 timesteps\n",
      "Episode finished after 194 timesteps\n",
      "Episode finished after 122 timesteps\n",
      "Episode finished after 109 timesteps\n",
      "Episode finished after 102 timesteps\n",
      "Episode finished after 144 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 173 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 139 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 98 timesteps\n",
      "Episode finished after 83 timesteps\n",
      "Episode finished after 417 timesteps\n",
      "Episode finished after 283 timesteps\n",
      "Episode finished after 168 timesteps\n",
      "Episode finished after 163 timesteps\n",
      "Episode finished after 290 timesteps\n",
      "Episode finished after 103 timesteps\n",
      "Episode finished after 120 timesteps\n",
      "Episode finished after 110 timesteps\n",
      "Episode finished after 258 timesteps\n",
      "Episode finished after 92 timesteps\n",
      "Episode finished after 160 timesteps\n",
      "Episode finished after 225 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 271 timesteps\n",
      "Episode finished after 104 timesteps\n",
      "Episode finished after 184 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 126 timesteps\n",
      "Episode finished after 110 timesteps\n",
      "Episode finished after 453 timesteps\n",
      "Episode finished after 149 timesteps\n",
      "Episode finished after 204 timesteps\n",
      "Episode finished after 100 timesteps\n",
      "Episode finished after 381 timesteps\n",
      "Episode finished after 167 timesteps\n",
      "Episode finished after 81 timesteps\n",
      "Episode finished after 93 timesteps\n",
      "Episode finished after 80 timesteps\n",
      "Episode finished after 230 timesteps\n",
      "Episode finished after 124 timesteps\n",
      "Episode finished after 94 timesteps\n",
      "Episode finished after 156 timesteps\n",
      "Episode finished after 252 timesteps\n",
      "Episode finished after 81 timesteps\n",
      "Episode finished after 142 timesteps\n",
      "Episode finished after 112 timesteps\n",
      "194.83\n"
     ]
    }
   ],
   "source": [
    "# render the model  \n",
    "time = []\n",
    "for i_episode in range(100):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        state = preprocess_state(observation)\n",
    "        action = np.argmax(model.predict(state)[0])\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            time.append((t+1))\n",
    "            break\n",
    "print(np.mean(time))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316, 201, 90, 232, 405, 82, 269, 256, 186, 88, 92, 84, 192, 213, 114, 248, 218, 78, 294, 96, 105, 301, 168, 245, 98, 200, 128, 191, 109, 192, 181, 218, 96, 162, 89, 241, 480, 267, 134, 500, 91, 404, 103, 313, 104, 161, 169, 236, 88, 167, 120, 194, 122, 109, 102, 144, 500, 173, 290, 139, 500, 98, 83, 417, 283, 168, 163, 290, 103, 120, 110, 258, 92, 160, 225, 500, 271, 104, 184, 500, 131, 126, 110, 453, 149, 204, 100, 381, 167, 81, 93, 80, 230, 124, 94, 156, 252, 81, 142, 112]\n"
     ]
    }
   ],
   "source": [
    "print time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
